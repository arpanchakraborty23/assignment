{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (767390265.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Boosting is ensemble method. where we use multipel weak learners for traning\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Boosting is ensemble method. where we use multipel weak learners for traning\n",
    "sequencelly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (372354608.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    advantges:\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "advantges: \n",
    "\t1. It combine multipel weak model to from a strong predictor.\n",
    "\t2. Boosting reduce bias by adjusting weights\n",
    "\t3. Boosting is robust to overfitting\n",
    "\n",
    "Disadvantages:\n",
    "\t1. Boosting algorithms can be computationally expensive and slow to train, especially with large datasets \n",
    "\t2. Boosting can be sensitive to noisy data and outliers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1: It assing a weight value for every intnces in dataset the weight will be \n",
    "similar for every data points.\n",
    "\n",
    "sstep2: constranct Desicion tree and find error value\n",
    "\n",
    "step3: now again weight uptded but error weight will be big compare to correctly\n",
    "predicted value.\n",
    "\n",
    "step4: crete new dataset for next model and pass to  them new dataset.from step 1 to 4\n",
    "   will repeat untill get minimal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting algorithms:\n",
    "\t\t\t1. Ada boosting\n",
    "\t\t\t2. Gradient Boosting\n",
    "\t\t\t3. Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Parameters:\n",
    "\t1. n_estimators : no of weak number\n",
    "\t2. Learning Rate : a regularaziation prameter\n",
    "\t3. Loss function : Minimize loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting alogorithm combine multpel weak lerner to stromg learner to a itertive\n",
    "process, where each learner train for minimize error of previous model .\n",
    "final result get by voting for clf and regression calculate avrage weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  initalize a weight for all data points and creat DT base lerner with\n",
    "stump 1\n",
    "\n",
    "2. Calculate Error  from DT  and found stump perfomnce\n",
    "\n",
    "4. now update weights of correclty pt and incorrectly predicted points.\n",
    "errors weights are incresing and correctly predicted pt weights are decressing.\n",
    "\n",
    "5. create randomly a new dataset with privious error data and pass to next model\n",
    "\n",
    "6. repert 1- 5 steps until get minimize loss\n",
    "7. finla output will be voting wegiht for clfierr and avarage weight for regrasssion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the AdaBoost algorithm, the loss function used is the exponential loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First it find the missclfified points by DT stump and calculate perfomance of stamp \n",
    "\n",
    "perfomance of stamp=1/2 ln[1- total errror/ total error]\n",
    "\n",
    "second it update weight by decressing and incresing stum\n",
    "correct point= sample weight* e^-(perfomance of stamp)\n",
    "\n",
    "missclified point =sample weight* e^-(perfomance of stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adding more estimators usually increases the overall accuracy of the model\n",
    "2. ore estimators help to reduce the bias of the model. Each new estimator is added to reduce the training error, so the model becomes more flexible and can capture more complex patterns in the data.\n",
    "\n",
    "3. : While more estimators can improve accuracy, there is also a risk of overfitting\n",
    "\n",
    "4. ore estimators mean more models to train, which increases the overall training time. This can be a significant drawback for very large datasets or when computational resources are limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
