{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8abeea68-a9e7-4edc-8cca-8df63ea28ccb",
   "metadata": {},
   "source": [
    "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distancemetric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59063f35-334f-44b3-b7f5-e3619052ed5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Euclidean distance is sensative to scale it squaring the features if we have large no of scale  features it dominte distance calculation.\n",
    "it can become less effective in high-dimensional spaces due to the curse of dimensionality,where distances between points tend to become more similar.\n",
    "\n",
    "Manhattan Distance is less sensitive to the scale of the features compared to Euclidean distance since it involves absolute values.\n",
    " It mitigates some issues related to the curse of dimensionality because it does not square the differences, making it less sensitive to the distribution of data.\n",
    "\n",
    "In datasets where features are on different scales or the importance of features varies significantly, Manhattan distance might perform better. \n",
    "if the data is well-scaled and distances between points are meaningful in a straight-line sense, Euclidean distance might be more effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38995c6c-79bc-4263-9f0a-f9102baabea4",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ec6c7-8473-4d8e-8233-76543315224a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The value of k determines how many neighbors to consider when making predictions\n",
    "Grid search is a systematic way of working through multiple combinations of parameter values, cross-validating each to determine which combination gives the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270d9ca-6fbd-4b17-8959-f446293acfac",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "# what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fab93a7b-66ee-4e63-955c-26f5debf6718",
   "metadata": {
    "tags": []
   },
   "source": [
    "Euclidean Distance :it effective for problems where the geometric distance between points is meaningful. Can be dominated by features with larger scales, so normalization is often required.\n",
    "\n",
    "Manhattan Distance: Sum of absolute differences. Less sensitive to outliers compared to Euclidean distance.\n",
    "\n",
    "\n",
    "If features are not normalized, those with larger scales can disproportionately influence the distance computation.Manhattan distance are less sensitive to features\n",
    "\n",
    "Manhattan distance often performs better in high dimensions because it considers the absolute differences, making it more robust.\n",
    "\n",
    "In image recognition, Euclidean distance may be more appropriate if the pixel values are normalized and represent spatial distances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018edc81-84bc-4551-a1f1-b29c68117bad",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "# the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2d3e6f9-9dee-4acc-a8b5-3151c98a42b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "K-Nearest Neighbors classifiers and regressors, several hyperparameters significantly affect the performance of the model. Here are some common hyperparameters\n",
    "\n",
    "K:  The number of nearest neighbors to consider for making predictions.small k can lead to high variance and overfitting, capturing noise in the data. A large k can lead to high bias and underfitting, oversimplifying the decision boundary.\n",
    "\n",
    "Distance Metric: The metric used to measure the distance between points.e choice of metric can influence model performance depending on the data characteristics.\n",
    "\n",
    "Algorithm: The algorithm used to compute the nearest neighbors.he choice of algorithm can affect the computational efficiency and scalability. For large datasets, tree-based method can be more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca37c93-2ddc-49da-b177-af27bd18b812",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "# techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d86852c6-8ccb-48bb-a042-259f3f3df15f",
   "metadata": {
    "tags": []
   },
   "source": [
    "KNN is a distance based alogorithm if we use a large size of dataset it try to calculate distance between new data and every train data to calculte every data it is very\n",
    "cost expensive and perfomance get degraded.\n",
    "\n",
    "optimization:\n",
    "    1. Randomly select a subset of the training data. This can help reduce the size of the training set while retaining a representative sample.\n",
    "    2.Create multiple subsets of the training data by sampling with replacement and train an ensemble of KNN models. This can improve robustness and generalization.\n",
    "    3. Reduces the number of dimensions by projecting the data onto the principal components that explain the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38d647-b528-4149-9259-8b9f1686eca5",
   "metadata": {},
   "source": [
    "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "# overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df06ae09-95d0-42be-96de-034f8e1cbef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "KNN is a distance based alogorithm if we use a large size of dataset it try to calculate distance between new data and every train data to calculte every data it is very\n",
    "cost expensive\n",
    "\n",
    "sol:Use efficient data structures like KD-Trees or Ball Trees to speed up neighbor searches.\n",
    "\n",
    " In high-dimensional spaces, distances between points become less meaningful, and the performance of KNN can degrade.\n",
    "sol : Apply dimensionality reduction techniques to reduce dimensionality.\n",
    "\n",
    "KNN can struggle with imbalanced datasets where some classes are underrepresented, leading to biased predictions.\n",
    "sol:Use techniques like SMOTE to balance datasets\n",
    "\n",
    "overcome these drawbacks:\n",
    "1.Normalize or standardize features to bring them to a common scale\n",
    "2. Apply SMOTE or other resampling techniques to balance the class distribution in the training set.\n",
    "3. Apply hyperprameter tuning with diffrent parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e91a04e-2f4e-4516-a399-6439dd008edf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
