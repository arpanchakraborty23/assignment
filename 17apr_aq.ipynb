{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involves predicting continuous numerical values. It is based on the principle of boosting, where multiple weak learners  are combined to form a strong learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=fetch_california_housing()\n",
    "x=pd.DataFrame(data.data,columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc        0\n",
       "HouseAge      0\n",
       "AveRooms      0\n",
       "AveBedrms     0\n",
       "Population    0\n",
       "AveOccup      0\n",
       "Latitude      0\n",
       "Longitude     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6876387732987639\n",
      "MSE  0.41645148123643827\n",
      "MAE  0.4287517906499025\n",
      "RMsE  0.6453305209243076\n"
     ]
    }
   ],
   "source": [
    "model=GradientBoostingRegressor(loss='absolute_error',n_estimators=50)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "    \n",
    "acc=r2_score(y_test,y_pred)\n",
    "print(acc)\n",
    "print( 'MSE ',mean_squared_error(y_test,y_pred))\n",
    "print( 'MAE ',mean_absolute_error(y_test,y_pred))\n",
    "print('RMsE ',np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8159953631176897\n",
      "MSE  0.24532175261721692\n",
      "MAE  0.31332651183341986\n",
      "RMsE  0.4952996594156076\n"
     ]
    }
   ],
   "source": [
    "model=GradientBoostingRegressor(loss='absolute_error',n_estimators=100,learning_rate=0.1,max_depth=8)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred=model.predict(x_test)\n",
    "    \n",
    "acc=r2_score(y_test,y_pred)\n",
    "print(acc)\n",
    "print( 'MSE ',mean_squared_error(y_test,y_pred))\n",
    "print( 'MAE ',mean_absolute_error(y_test,y_pred))\n",
    "print('RMsE ',np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Weak learners are typically fast to train and require less computational power.\n",
    "By combining many weak learners, each focusing on different aspects of the data, the final model captures a wide range of patterns and reduces both bias and variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2631630707.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[33], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    1. Ensemble Learning: Ensemble learning is the process of combining multiple models to produce a single, more powerful model.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. Ensemble Learning: Ensemble learning is the process of combining multiple models to produce a single, more powerful model.\n",
    "\n",
    "2. Boosting:  It works by training a sequence of models, where each model attempts to correct the errors of the previous one\n",
    "\n",
    "3. Weak Learners: In Gradient Boosting, weak learners are typically simple models like shallow decision trees. Each weak learner may perform only slightly better than random guessing, but their sequential combination leads to a powerful model.\n",
    "4. Gradient Descent: The \"gradient\" in Gradient Boosting refers to the use of gradient descent optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (677195658.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[34], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    3.rain a weak learner  on the residuals. The weak learner is trained to predict these residuals.\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "1. The process starts with dump prediction,  such as predicting the mean of the target variable.\n",
    "\n",
    "2. Calculate the residuals  between the current predictions and the actual target values.\n",
    "\n",
    "3.rain a weak learner  on the residuals. The weak learner is trained to predict these residuals.\n",
    "\n",
    "4.Combine the predictions of the weak learner with the predictions from all previous models\n",
    "5. Repeat steps 2-4 for a specified number of iterations or until a convergence criterion is met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 4) (10230288.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    2. Compute the Negative Gradient: Calculate the negative gradient of the loss function with respect to the initial model's prediction\u001b[0m\n\u001b[1;37m                                                                                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
     ]
    }
   ],
   "source": [
    "1. Define the Loss Function: Start by defining a differentiable loss function L(y,y_hat)\n",
    "hat measures the error between the true target values nd model y_pred\n",
    "\n",
    "2. Compute the Negative Gradient: Calculate the negative gradient of the loss function with respect to the initial model's prediction\n",
    "dL/dy_0\n",
    "\n",
    "3. Train a Weak Learner: Fit a weak learner to predict the negative gradients (pseudo-residuals). This weak learner is trained to minimize the residuals left by the current model\n",
    "\n",
    "4. Update the Model: Update the current model by adding the predictions of the weak learner, scaled by a learning rate \n",
    " gemma\n",
    "\n",
    "y_hat1=y_hat10 + gemma *f1(x)\n",
    "\n",
    "5. Repeat the Process: Repeat steps 3-5 to build a sequence of weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3804912966.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[36], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    The final ensemble model is the sum of the initial prediction and the predictions from all weak learners:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The final ensemble model is the sum of the initial prediction and the predictions from all weak learners:\n",
    "\n",
    "y_finl= y_hat0 +gemma summason(f(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
