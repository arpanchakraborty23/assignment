{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41358846-66f3-47fc-9531-54b4452d9acc",
   "metadata": {},
   "source": [
    "# Q1. What is an activation function in the context of artificial neural networks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b9311c6-d8af-442c-86d4-4cbda4474b9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "activation function is a mathamatical function that use for add non-liearty to artificial neural network without activation function\n",
    "neural network works as single layer proceptron.It also determine nurons should activate or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56952d-1b5f-4b6f-9396-b9f4e8a11a00",
   "metadata": {},
   "source": [
    "# Q2. What are some common types of activation functions used in neural networks?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0d54422-241c-480d-b6b2-5c37be8c351d",
   "metadata": {},
   "source": [
    "Activation functions :\n",
    "    \n",
    "    sigmoid activation function: it is most commonly use activation function for binary problems\n",
    "    \n",
    "    tanh activation function: Zero-centered, which can lead to better convergence compared to the sigmoid function.\n",
    "        \n",
    "    Liner activation function: y=4x it use for regression task.\n",
    "    \n",
    "    relu: most commonly use activation function on hidden layers.it replace all negative numbers to zero\n",
    "    \n",
    "    softmax activation function: Used in the output layer for multi-class classification problems to provide a probability distribution over classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0785b7d2-8b4a-427a-871a-ce0548ec6ecd",
   "metadata": {},
   "source": [
    "# Q3. How do activation functions affect the training process and performance of a neural network?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50478516-4cfe-4f90-92dc-ecb940d4952c",
   "metadata": {},
   "source": [
    "activation functions effect on  neural networks:\n",
    "    1. non-lieaty: activation functions enabels nureal network to learn complex data,without activation function networks works as liner model regurd less\n",
    "    of multipel hidden layers.\n",
    "    \n",
    "    2. gradient flow:activation functions affect how gradients are propagated through the network during backpropagation. Functions with large derivatives can lead to exploding gradients and\n",
    "    small derivatives can lead to vanishing gradients.\n",
    "    \n",
    "    3.convergance speed: The choice of activation function can impact the speed at which a network converges to global minima\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147db18-085b-4992-ad82-0357c8bbc45d",
   "metadata": {},
   "source": [
    "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88d55bb5-8fbb-4e73-8341-6337504e0ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sigmoid activation function is non linear activation function it is commnly use for binary outcome problems.sigmoid activation function use squashing \n",
    "technique that range output o-1 .\n",
    "\n",
    "advantages:\n",
    "    1. it is diffrancable and smooth gradient.\n",
    "    2. output can be probablistic\n",
    "    \n",
    "Disadvantages:\n",
    "    1. darivatibe of sigmoid range 0-0.25 ,so it can lead vansing gradient problem\n",
    "    2. output is not zero centric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632c43a-3a08-428f-a94a-465f569ab331",
   "metadata": {},
   "source": [
    "# Q5.What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95305819-8dfd-44ad-90f8-1acbe82d6796",
   "metadata": {
    "tags": []
   },
   "source": [
    "Rectified linear unit in sort Relu is a non linear activation function.it mainly use in hidden layers.Relu solve limitation of\n",
    "sigmoid and tanh vansing gradinent problem.it replace negative values into zero that speed up our convargance speeed. But sometime poor weight initialization\n",
    "can lead to more neurons dead.\n",
    "\n",
    "formula=max(0,x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93ee33-3987-4d69-a4d9-7644b5c6aab6",
   "metadata": {},
   "source": [
    "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cbe078d-fa57-483e-81c2-956cbc77fe0e",
   "metadata": {},
   "source": [
    "Relu mainly use in hidden layers unlike sigmoid if we use it in hidden layer it sigmoid can lead vanisng gradient problem.\n",
    "it replace negative values into zero that speed up our convargance speeed and also take less memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73f088-38da-4e89-9388-da9cd9a0ea26",
   "metadata": {},
   "source": [
    "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "raw",
   "id": "af3f1938-2459-45cb-91ff-bd1b67f5cf41",
   "metadata": {},
   "source": [
    "The Leaky ReLU is an activation function used in neural networks, designed to solve issues faced by the  ReLU, specifically the \"dying ReLU\" problem. It introduces a small, non-zero slope for negative input values, allowing some gradient to pass through even when the neuron is not activated.\n",
    "\n",
    "Leaky ReLU maintains a non-zero gradient for negative inputs. This helps prevent gradients from diminishing too quickly during backpropagation, enabling effective learning across all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110516-8f62-456e-95c0-398912db8bf2",
   "metadata": {},
   "source": [
    "# Q8. What is the purpose of the softmax activation function? When is it commonly used?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f222f0a8-14af-41f1-9b53-42abf2ccb1b9",
   "metadata": {},
   "source": [
    "softmax activation function is a non linear activation function that use for final output layer for multiclass classification\n",
    "problem and it diffrantable. softmax transforms the output of a neural network into a probability distribution, where the sum of all probabilities equals 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bfe08-7648-4040-b911-4ebde4a3fc5b",
   "metadata": {},
   "source": [
    "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1777e6e-f934-4466-b14d-f6e897a4ec98",
   "metadata": {},
   "source": [
    "The hyperbolic tangent (tanh) activation function is a widely used activation function in neural networks, particularly in hidden layers. It is similar to the sigmoid function.The graph of the tanh function is an \"S\" shape, similar to the sigmoid function, but it spans from -1 to 1.\n",
    "The outputs of tanh are centered around zero, which helps the gradients during backpropagation remain balanced, leading to more efficient and faster convergence.But  mejore disadvantage is it also face vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee344a90-00ac-4e0a-8a1f-0c2b947e60bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
