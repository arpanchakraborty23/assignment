{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343e2f1a-a56d-4c62-8eb4-f2dcf631bb33",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd3f02bc-148e-4aa6-be80-9c542cd30749",
   "metadata": {},
   "source": [
    "R-squared is perfomance metrics for regresion models except logistic regression.\n",
    "\n",
    "it calculate by  squred sum resudels / squared sum of total \n",
    "\n",
    "R-squared=1- (sum(y_actuatl - y_predict)^2/ sum(y_actual - y_actual_mean)^2)\n",
    "\n",
    "It reprasent perfomance of the model. R squared value =0.89 it's mean our model perfomence is 89%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba4f46-91e5-49c5-bdc8-7f4df50c18fd",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "raw",
   "id": "56184495-81de-4eea-94c8-1048eb1a8191",
   "metadata": {
    "tags": []
   },
   "source": [
    "The masure Disadantage of R-square when adding new feature that new feature might be not heighly importent for prediction (irrelevant feature) but R-square \n",
    "constant incresess which is incorrect , that's why adjusted R-squared came to the picture\n",
    "\n",
    "adjusted R-squared=1 - (1-r2)(n-1)/n-p-1\n",
    "r2=r2 square\n",
    "n= total no of data pt\n",
    "p= features\n",
    "\n",
    "as feature incresses(p) denominator will decrease.n-1 , 1-r2 remain constant\n",
    "when we subtract this from one then the resultant score will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29ffdf-aa7a-4944-ae26-03bc734e8e6e",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cac99b70-2bab-4ee4-9834-43ad2db42eea",
   "metadata": {},
   "source": [
    "1. Adjusted R-squared is particularly useful when comparing models with different numbers of predictors, as it adjusts for the number of predictors and the sample size\n",
    "2. When performing model selection, such as adding or removing predictors, adjusted R-squared helps determine whether the added complexity of a model is justified by a better fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd71e96-f9a0-4b18-bf47-e785ad6c9b93",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "## calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60c0d726-98ed-46ac-b6a4-a7648ffb4eb4",
   "metadata": {},
   "source": [
    "If we don't measure failure we can't get  success\n",
    "Machine learning model cannot have 100 per cent efficiency otherwise the model is known as a biased model.\n",
    "generalized model we require to Evaluate the model on different metrics which helps us to better optimize the performance, fine-tune it, and obtain a better result.\n",
    "\n",
    "MSE is the defult cost function for liner regrassion . it use for mesure the error rate of a model\n",
    "\n",
    "MSE=1/n sum(y_actual - y_predict)\n",
    "\n",
    "MAE  calculates the absolute difference between actual and predicted values.It is most Robust to outliers.\n",
    "\n",
    "MAE= 1/n *sum| y_actual - y_predict|\n",
    "\n",
    "RMSE it is a simple square root of mean squared error.\n",
    "\n",
    "RMSE= sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f9ee5-2b4e-40a4-8214-ea2292ff82af",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc3678f7-9da1-4135-9218-04cd8409220f",
   "metadata": {
    "tags": []
   },
   "source": [
    "                 Advantages                                                                      Disadvantages\n",
    "\n",
    "    \n",
    "MAE :- 1. It is robust to outliers.                                            1. It use subgradient decent so time complex\n",
    "\n",
    "\n",
    "MSE : - 1.Easy to diffratiable                                                 1. not robust to outliers\n",
    "        2. it has only global minima\n",
    "    \n",
    "Rmse: - 1. unit's are same                                                     1. not robust to outliers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a61a811-b497-4b1b-89b4-18c8971df63b",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "## it more appropriate to use?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36b0709c-d18d-498e-8f99-8196a26d3b48",
   "metadata": {},
   "source": [
    "Lasso is L1 regularization thechnique of liner regression model. It pervent model to get overfitting by adding penalty tearm as | slop| and create a complex model that can predict complex data.\n",
    "It use feature seclation process to remove less importent features during train and prediction. Lasso regularization adds a penalty equal to the absolute value of the coefficients to the \n",
    "regression model's objective function, leading to sparse models with some coefficients set to zero\n",
    "\n",
    "\n",
    "Ridge regularization, also known as L2 regularization, is another technique to prevent overfitting, but it differs from Lasso in how the penalty is applied.\n",
    "iRidge regularization adds a penalty equal to the square of the coefficients, resulting in models where coefficients are shrunk but none are exactly zero\n",
    "\n",
    "\n",
    "When we have large no feature lasso is a good option for them and Ridge is good for multicolinarty problem need to predict all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2652c-de7e-438b-9993-c4e39d8a7dfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "raw",
   "id": "32041578-36d5-4e03-ad33-5bc5320ac39f",
   "metadata": {},
   "source": [
    "Regularization model help to prevent overfitting in machine learning by adding a penalty term. this penalty tearm control complxity of model ,ensure that it can predict\n",
    "unseen data accuartly\n",
    "ex-\n",
    "We have 100 data points.\n",
    "10 features, with a mixture of relevant and irrelevant features.\n",
    "\n",
    "Without Regularization:\n",
    "\n",
    "We fit a linear regression model to the data.\n",
    "The model tries to minimize the residual sum of squares  without any penalty on the size of the coefficients.\n",
    "If some features are irrelevant or highly collinear, the model might assign large coefficients to these features to fit the training data closely, leading to overfitting.\n",
    "\n",
    "With Regularization:\n",
    "\n",
    "Lasso Regression adds a penalty proportional to the sum of the absolute values of the coefficients.\n",
    "Some coefficients might be set to zero, effectively performing feature selection.\n",
    "\n",
    "Ridge Regression adds a penalty proportional to the sum of the squared values of the coefficients.\n",
    "All coefficients are shrunk towards zero but none are eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90255a-34df-4d15-8cd0-217ede485a72",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "# choice for regression analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f168769-9c7e-475f-972a-02f81059c1cd",
   "metadata": {},
   "source": [
    "1.Regularized Linear Models can make model simple if Regularized parameter is too large it create model unstable and lead's to underfitting\n",
    "\n",
    "2.Regularization can be computationally expensive, especially for multidimensional data.\n",
    "\n",
    "3. outliers can affect cofficient and intercept tha can shift the regression line eading to inaccurate predictions.\n",
    "4. Multicollinearity can lead to unstable and unreliable coefficient estimates, making it difficult to interpret the results of the model accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d535fa-8161-49bc-a7dd-ef1ef5091495",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "## Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "# performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c510a97f-c8cf-4be1-8b77-f42807a7a1ba",
   "metadata": {},
   "source": [
    "1.If Model A’s RMSE of 10 is significantly higher than its MAE, it might indicate the presence of outliers. If we assume Model A’s MAE is less than 10 ,\n",
    "and Model B has an MAE of 8, Model B might be handling outliers better or have fewer outliers.\n",
    "\n",
    "2.if typical performance without the influence of outliers is more important, Model B’s MAE of 8 suggests it might have a more consistent error pattern.\n",
    "\n",
    "RmSE not robust to outliers is squreing them\n",
    "MAE Might not fully capture the variance in error distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cd6c9-1135-4ca9-9a05-ebbbdbe70113",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "## ses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e9c4c88-b62c-4928-9485-95ccf4bdd29f",
   "metadata": {},
   "source": [
    "1. Model A (ridge) has a lower λ (0.1) compared to Model B’s higher λ (0.5). This means Model B(Lasso) is applying a stronger penalty, Lasso use feature selction process\n",
    "it can lead maxmimum feature to zero and it might exclude some relevant feature.\n",
    "\n",
    "2.we should compare the models using the same performance metrics such as \n",
    "Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) on  test dta.\n",
    "\n",
    "Ridge Regulariztion:\n",
    "    \n",
    "    1.Ridge regularization does not perform feature selection since it shrinks coefficients but does not set any to zero. \n",
    "This can be a limitation when you want to identify and exclude irrelevant features.\n",
    "\n",
    "    2. When the number of features is large, it can be challenging to understand the influence of each predictor.\n",
    "    \n",
    "Lasso Regulariztion:\n",
    "    1.Lasso performs automatic feature selection by setting some coefficients to exactly zero. While this can simplify the model, \n",
    "it might exclude some relevant features, especially if the regularization parameter is too high\n",
    "    \n",
    "    2.Lasso can be unstable in selecting variables, especially when there are high correlations between predictors. Small changes in the data can lead to different sets of selected features."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1258406b-0f6f-4a9f-8e7d-9d2c45ae2fbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
